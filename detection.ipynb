{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box\n",
    "from utils.plots import colors, plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_extract_faces(source, weights):\n",
    "    \n",
    "    # yolov7 setup\n",
    "    imgsz = 640\n",
    "    kpt_label = 5\n",
    "    conf_thres = 0.25\n",
    "    iou_thres = 0.45\n",
    "    classes = 0\n",
    "    line_thickness = 3\n",
    "    project = 'runs/detect'\n",
    "    name = 'exp'\n",
    "    device = 'cpu'\n",
    "\n",
    "    view_img = False\n",
    "    save_txt = False\n",
    "    save_txt_tidl = False\n",
    "    nosave = False\n",
    "    exist_ok = False\n",
    "    augment = False\n",
    "    agnostic_nms = False\n",
    "    save_crop = True\n",
    "    save_conf = False\n",
    "    hide_labels = False\n",
    "    hide_conf = False\n",
    "    \n",
    "\n",
    "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "\n",
    "    # Directories\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "    (save_dir / 'labels' if (save_txt or save_txt_tidl) else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device(device)\n",
    "    half = device.type != 'cpu' and not save_txt_tidl  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    if isinstance(imgsz, (list,tuple)):\n",
    "        assert len(imgsz) ==2; \"height and width of image has to be specified\"\n",
    "        imgsz[0] = check_img_size(imgsz[0], s=stride)\n",
    "        imgsz[1] = check_img_size(imgsz[1], s=stride)\n",
    "    else:\n",
    "        imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Second-stage classifier\n",
    "    classify = False\n",
    "    if classify:\n",
    "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        view_img = check_imshow()\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    t0 = time.time()\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=augment)[0]\n",
    "        print(pred[...,4].max())\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms, kpt_label=kpt_label)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Apply Classifier\n",
    "        if classify:\n",
    "            pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "            else:\n",
    "                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # img.jpg\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                scale_coords(img.shape[2:], det[:, :4], im0.shape, kpt_label=False)\n",
    "                scale_coords(img.shape[2:], det[:, 6:], im0.shape, kpt_label=kpt_label, step=3)\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, 5].unique():\n",
    "                    n = (det[:, 5] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for det_index, (*xyxy, conf, cls) in enumerate(reversed(det[:,:6])):\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                        with open(txt_path + '.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                        c = int(cls)  # integer class\n",
    "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                        kpts = det[det_index, 6:]\n",
    "                        plot_one_box(xyxy, im0, label=label, color=colors(c, True), line_thickness=line_thickness, kpt_label=kpt_label, kpts=kpts, steps=3, orig_shape=im0.shape[:2])\n",
    "                        if save_crop:\n",
    "                            save_one_box(xyxy, im0s, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "\n",
    "                if save_txt_tidl:  # Write to file in tidl dump format\n",
    "                    for *xyxy, conf, cls in det_tidl:\n",
    "                        xyxy = torch.tensor(xyxy).view(-1).tolist()\n",
    "                        line = (conf, cls,  *xyxy) if save_conf else (cls, *xyxy)  # label format\n",
    "                        with open(txt_path + '.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "            # Print time (inference + NMS)\n",
    "            print(f'{s}Done. ({t2 - t1:.3f}s)')\n",
    "\n",
    "            # Stream results\n",
    "            if view_img:\n",
    "                cv2.imshow(str(p), im0)\n",
    "                cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path != save_path:  # new video\n",
    "                        vid_path = save_path\n",
    "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                            vid_writer.release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                            save_path += '.mp4'\n",
    "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                    vid_writer.write(im0)\n",
    "\n",
    "    if save_txt or save_txt_tidl or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt or save_txt_tidl else ''\n",
    "        print(f\"Results saved to {save_dir}{s}\")\n",
    "\n",
    "    print(f'Done. ({time.time() - t0:.3f}s)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = './test_classroom.jpg'\n",
    "weights_path = './yolov7-face.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv7  2023-4-16 torch 2.0.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "c:\\Users\\h4rip\\anaconda3\\envs\\face\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 315 layers, 36562547 parameters, 0 gradients, 103.4 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 d:\\projects\\classroom_face_detection\\face_detection\\test_classroom.jpg: tensor(0.84553)\n",
      "640x640 100 faces, Done. (0.692s)\n",
      "Results saved to runs\\detect\\exp5\n",
      "Done. (1.192s)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    detect_and_extract_faces(source_path, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
